{% macro sentencecase(text) %}
    {{- text[0]|upper}}{{text[1:] -}}
{% endmacro %}
{% macro declare_members(NODE) %}
    {%- for CHILD in NODE.children %}
    {%- if TYPES.GROUP in CHILD.type %}
    self._{{ CHILD.name }} = AbsentGroup  # {{ CHILD.type }}
    {% else  %}
    self._{{ CHILD.name }} = AbsentDataset  # {{ CHILD.type }}
    {% endif -%}    
    {% endfor %}
    self._snirf_names = [{% for CHILD in NODE.children %}'{{ CHILD.name }}', {% endfor -%}]
{% endmacro %}
{% macro init_members(NODE) %}
    self._indexed_groups = []
    {% for CHILD in NODE.children %}
    {% if TYPES.INDEXED_GROUP in CHILD.type %}
    self.{{ CHILD.name }} = {{ sentencecase(CHILD.name) }}(self, self._cfg)  # Indexed group
    self._indexed_groups.append(self.{{ CHILD.name }})
    {% elif TYPES.GROUP in CHILD.type %}
    if '{{ CHILD.name }}' in self._h:
        self._{{ CHILD.name }} = {{ sentencecase(CHILD.name) }}(self._h['{{ CHILD.name }}'].id, self._cfg)  # Group
    else:
        self._{{ CHILD.name }} = {{ sentencecase(CHILD.name) }}(self.location + '/' + '{{ CHILD.name }}', self._cfg)  # Anonymous group (wrapper only)
    {% else %}
    if '{{ CHILD.name }}' in self._h:
        if not self._cfg.dynamic_loading:
        {% if (TYPES.ARRAY_1D in CHILD.type) or (TYPES.ARRAY_2D in CHILD.type) %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            self._{{ CHILD.name }} = _read_string_array(self._h['{{ CHILD.name }}'])
            {% elif TYPES.INT_VALUE in CHILD.type %}
            self._{{ CHILD.name }} = _read_int_array(self._h['{{ CHILD.name }}'])
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            self._{{ CHILD.name }} = _read_float_array(self._h['{{ CHILD.name }}'])
            {% else %}
            self._{{ CHILD.name }} = _read_dataset(self._h['{{ CHILD.name }}'])
            {% endif %}
        {% else %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            self._{{ CHILD.name }} = _read_string(self._h['{{ CHILD.name }}'])
            {% elif TYPES.INT_VALUE in CHILD.type %}
            self._{{ CHILD.name }} = _read_int(self._h['{{ CHILD.name }}'])
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            self._{{ CHILD.name }} = _read_float(self._h['{{ CHILD.name }}'])
            {% else %}
            self._{{ CHILD.name }} = _read_dataset(self._h['{{ CHILD.name }}'])
            {% endif %}
        {% endif %}
        else:  # if the dataset is found on disk but dynamic_loading=True
            self._{{ CHILD.name }} = PresentDataset
    else:  # if the dataset is not found on disk
        self._{{ CHILD.name }} = AbsentDataset
    {% endif %}
    {% endfor %}
    {% if NODE.name in UNSPECIFIED_DATASETS_OK %}
    self._unspecified_names = []
    # Unspecified datasets are not properties and unaffected by dynamic_loading
    for key in self._h.keys():
        # If the name isn't specified 
        if key not in self._snirf_names and not any([key in indexed_group for indexed_group in self._indexed_groups]):
            setattr(self, key, _read_dataset(self._h[key]))
            self._unspecified_names.append(key)
    {% endif %}            
{% endmacro %}
{% macro gen_properties(NODE) %}
{% for CHILD in NODE.children %}
    @property
    def {{ CHILD.name }}(self):
    {% if TYPES.INDEXED_GROUP in CHILD.type %}
        return self._{{ CHILD.name }}
    {% elif TYPES.GROUP in CHILD.type %}
        if type(self._{{ CHILD.name }}) is type(AbsentGroup):
            return None
        return self._{{ CHILD.name }}
    {% else %}
        if type(self._{{ CHILD.name }}) is type(AbsentDataset):
            return None
        if type(self._{{ CHILD.name }}) is type(PresentDataset):
        {% if (TYPES.ARRAY_1D in CHILD.type) or (TYPES.ARRAY_2D in CHILD.type) %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            return _read_string_array(self._h['{{ CHILD.name }}'])
            {% elif TYPES.INT_VALUE in CHILD.type %}
            return _read_int_array(self._h['{{ CHILD.name }}'])
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            return _read_float_array(self._h['{{ CHILD.name }}'])
            {% else %}
            return _read_dataset(self._h['{{ CHILD.name }}'])
            {% endif %}
        {% else %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            return _read_string(self._h['{{ CHILD.name }}'])
            {% elif TYPES.INT_VALUE in CHILD.type %}
            return _read_int(self._h['{{ CHILD.name }}'])
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            return _read_float(self._h['{{ CHILD.name }}'])
            {% else %}
            return _read_dataset(self._h['{{ CHILD.name }}'])
            {% endif %}
        {% endif %}
            self._cfg.logger.info('Dynamically loaded %s/{{ CHILD.name }} from %s', self.location, self.filename)
        return self._{{ CHILD.name }}
    {% endif %}

    @{{ CHILD.name }}.setter
    def {{ CHILD.name }}(self, value):
        self._{{ CHILD.name }} = value
        # self._cfg.logger.info('Assignment to %s/{{ CHILD.name }} in %s', self.location, self.filename)

    @{{ CHILD.name }}.deleter
    def {{ CHILD.name }}(self):
        {% if TYPES.INDEXED_GROUP in CHILD.type %}
        raise AttributeError('IndexedGroup ' + str(type(self._{{ CHILD.name }})) + ' cannot be deleted')
        {% elif TYPES.GROUP in CHILD.type %}
        self._{{ CHILD.name }} = AbsentGroup
        {% else %}
        self._{{ CHILD.name }} = AbsentDataset
        {% endif %}
        self._cfg.logger.info('Deleted %s/{{ CHILD.name }} from %s', self.location, self.filename)

    {% endfor %}
{% endmacro %}
{% macro gen_writer(NODE) %}
    def _save(self, *args):
        if len(args) > 0 and type(args[0]) is h5py.File:
            file = args[0]
            if self.location not in file:
                file.create_group(self.location)
                # self._cfg.logger.info('Created Group at %s in %s', self.location, file)
        else:
            if self.location not in file:
                # Assign the wrapper to the new HDF5 Group on disk
                self._h = file.create_group(self.location)
                # self._cfg.logger.info('Created Group at %s in %s', self.location, file)
            if self._h != {}:
                file = self._h.file
            else:
                raise ValueError('Cannot save an anonymous ' + self.__class__.__name__ + ' instance without a filename')
{% for CHILD in NODE.children %}
    {% if TYPES.INDEXED_GROUP in CHILD.type %}
        self.{{ CHILD.name }}._save(*args)
    {% elif TYPES.GROUP in CHILD.type %}
        if type(self._{{ CHILD.name }}) is type(AbsentGroup) or self._{{ CHILD.name }}.is_empty():
            if '{{ CHILD.name }}' in file:
                del file['{{ CHILD.name }}']
                self._cfg.logger.info('Deleted Group %s/{{ CHILD.name }} from %s', self.location, file)
        else:
            self.{{ CHILD.name }}._save(*args)
    {% else %}
        name = self.location + '/{{ CHILD.name }}'
        if type(self._{{ CHILD.name }}) not in [type(AbsentDataset), type(None)]:
            data = self.{{ CHILD.name }}  # Use loader function via getter
            if name in file:
                del file[name]
        {% if (TYPES.ARRAY_1D in CHILD.type) or (TYPES.ARRAY_2D in CHILD.type) %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            _create_dataset_string_array(file, name, data)
            {% elif TYPES.INT_VALUE in CHILD.type %}
            _create_dataset_int_array(file, name, data)
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            _create_dataset_float_array(file, name, data)
            {% else %}
            _create_dataset(file, name, data)
            {% endif %}
        {% else %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
            _create_dataset_string(file, name, data)
            {% elif TYPES.INT_VALUE in CHILD.type %}
            _create_dataset_int(file, name, data)
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
            _create_dataset_float(file, name, data)
            {% else %}
            _create_dataset(file, name, data)
            {% endif %}
        {% endif %}
            # self._cfg.logger.info('Creating Dataset %s in %s', name, file)
        else:   
            if name in file:
                del file[name]
                self._cfg.logger.info('Deleted Dataset %s from %s', name, file)
    {% endif %}
    {% endfor %}
    {% if NODE.name in UNSPECIFIED_DATASETS_OK %}
        for unspecified_name in self._unspecified_names:
            name = self.location + '/' + unspecified_name
            if name in file:
                del file[name]
                self._cfg.logger.info('Deleted Dataset %s from %s', name, file)
            try:
                data = getattr(self, unspecified_name)
            except AttributeError:  # Dataset was deleted
                continue
            _create_dataset(file, name, data)
            
    {% endif %}
{% endmacro %}
{% macro gen_validator(NODE) %}
    def _validate(self, result: ValidationResult):
        # Validate unwritten datasets after writing them to this tempfile
        tmp = h5py.File(TemporaryFile(), 'w')
    {% for CHILD in NODE.children %}
        name = self.location + '/{{ CHILD.name }}'
    {% if TYPES.INDEXED_GROUP in CHILD.type %}
        {% if TYPES.REQUIRED in CHILD.type %}
        if len(self._{{ CHILD.name }}) == 0:
            result._add(name, 'REQUIRED_INDEXED_GROUP_EMPTY')
        {% else %}
        if len(self._{{ CHILD.name }}) == 0:
            result._add(name, 'OPTIONAL_INDEXED_GROUP_EMPTY')
        {% endif %}
        else:
            self.{{ CHILD.name }}._validate(result)
    {% elif TYPES.GROUP in CHILD.type %}
        if type(self._{{ CHILD.name }}) in [type(AbsentGroup), type(None)]:
        {% if TYPES.REQUIRED in CHILD.type %}
            result._add(name, 'REQUIRED_GROUP_MISSING')
        {% else %}
            result._add(name, 'OPTIONAL_GROUP_MISSING')
        {% endif %}
        else:
            self._{{ CHILD.name }}._validate(result)
    {% else %}
        if type(self._{{ CHILD.name }}) in [type(AbsentDataset), type(None)]:
        {% if TYPES.REQUIRED in CHILD.type %}
            result._add(name, 'REQUIRED_DATASET_MISSING')
        {% else %}
            result._add(name, 'OPTIONAL_DATASET_MISSING')
        {% endif %}
        else:
            try:
                if type(self._{{ CHILD.name }}) is type(PresentDataset):
                    dataset = self._h['{{ CHILD.name }}']
                else:
        {% if TYPES.ARRAY_2D in CHILD.type %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
                    dataset = _create_dataset_string_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_string_array(dataset, ndims=[2]))
            {% elif TYPES.INT_VALUE in CHILD.type %}
                    dataset = _create_dataset_int_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_int_array(dataset, ndims=[2]))
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
                    dataset = _create_dataset_float_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_float_array(dataset, ndims=[2]))
            {% else %}
                ### TEMPLATE ERROR: NO VALIDATOR FUNCTION MATCHING {{ CHILD.name }}
            {% endif %}
        {% elif TYPES.ARRAY_1D in CHILD.type %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
                    dataset = _create_dataset_string_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_string_array(dataset, ndims=[1]))
            {% elif TYPES.INT_VALUE in CHILD.type %}
                    dataset = _create_dataset_int_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_int_array(dataset, ndims=[1]))
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
                    dataset = _create_dataset_float_array(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_float_array(dataset, ndims=[1]))
            {% else %}
            ### TEMPLATE ERROR: NO VALIDATOR FUNCTION MATCHING {{ CHILD.name }}
            {% endif %}
        {% else %}
            {% if TYPES.VARLEN_STRING in CHILD.type %}
                    dataset = _create_dataset_string(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_string(dataset))
            {% elif TYPES.INT_VALUE in CHILD.type %}
                    dataset = _create_dataset_int(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                {% if 'Index' in CHILD.name %}
                err_code = _validate_int(dataset)
                if _read_int(dataset) < 0 and err_code == 'OK':
                    result._add(name, 'NEGATIVE_INDEX')
                elif _read_int(dataset) == 0 and err_code == 'OK':
                    result._add(name, 'INDEX_OF_ZERO')
                else:
                    result._add(name, err_code)
                {% else %}
                result._add(name, _validate_int(dataset))
                {% endif %}
            {% elif TYPES.FLOAT_VALUE in CHILD.type %}
                    dataset = _create_dataset_float(tmp, '{{ CHILD.name }}', self._{{ CHILD.name }})
                result._add(name, _validate_float(dataset))
            {% else %}
            ### TEMPLATE ERROR: NO VALIDATOR FUNCTION MATCHING {{ CHILD.name }}
            {% endif %}
        {% endif %}
            except ValueError:  # If the _create_dataset function can't convert the data
                result._add(name, 'INVALID_DATASET_TYPE')
    {% endif %}
    {% endfor %}
    {% if not NODE.name in UNSPECIFIED_DATASETS_OK %}
        for key in self._h.keys():
            if not any([key.startswith(name) for name in self._snirf_names]):
                if type(self._h[key]) is h5py.Group:
                    result._add(self.location + '/' + key, 'UNRECOGNIZED_GROUP')
                elif type(self._h[key]) is h5py.Dataset:
                    result._add(self.location + '/' + key, 'UNRECOGNIZED_DATASET')
    {% endif %}
{% endmacro %}
{{ HEADER }}

# generated by {{ USER }} on {{ DATE }}
# version {{ VERSION }} SNIRF specification parsed from {{ SPEC_SRC }}


{% for GROUP in GROUPS %}
class {{ sentencecase(GROUP.name) -}}(Group):

    def __init__(self, var, cfg: SnirfConfig):
        super().__init__(var, cfg)
    {{ declare_members(GROUP) | indent }}
    {{ init_members(GROUP) | indent }}
{{ gen_properties(GROUP) }}
{{ gen_writer(GROUP) }}
{{ gen_validator(GROUP) }}


{% endfor %}
{% for INDEXED_GROUP in INDEXED_GROUPS %}
class {{ sentencecase(INDEXED_GROUP.name) -}}Element(Group):

    def __init__(self, gid: h5py.h5g.GroupID, cfg: SnirfConfig):
        super().__init__(gid, cfg)
    {{ declare_members(INDEXED_GROUP) | indent }}
    {{ init_members(INDEXED_GROUP) | indent }}
{{ gen_properties(INDEXED_GROUP) }}
{{ gen_writer(INDEXED_GROUP) }}
{{ gen_validator(INDEXED_GROUP) }}

class {{ sentencecase(INDEXED_GROUP.name) -}}(IndexedGroup):

    _name: str = '{{ INDEXED_GROUP.name }}'
    _element: Group = {{ sentencecase(INDEXED_GROUP.name) -}}Element

    def __init__(self, h: h5py.File, cfg: SnirfConfig):
        super().__init__(h, cfg)


{% endfor %}
class Snirf(Group):
    
    _name = '/'
    
    # overload
    def __init__(self, *args, dynamic_loading: bool = False, logfile: bool = False):
        self._cfg = SnirfConfig()
        self._cfg.dynamic_loading = dynamic_loading
        if len(args) > 0:
            path = args[0]
            if type(path) is str:
                if not path.endswith('.snirf'):
                    path = path + '.snirf'
                if logfile:
                    self._cfg.logger = _create_logger(path, path.split('.')[0] + '.log')
                else:
                    self._cfg.logger = _logger  # Use global logger
                if os.path.exists(path):
                    self._cfg.logger.info('Loading from file %s', path)
                    self._h = h5py.File(path, 'r+')
                else:
                    self._cfg.logger.info('Creating new file at %s', path)
                    self._h = h5py.File(path, 'w')
            else:
                raise TypeError(str(path) + ' is not a valid filename')
        else:
            self._cfg.logger = _logger
            self._cfg.logger.info('Created Snirf object based on tempfile')
            path = None
            self._h = h5py.File(TemporaryFile(), 'w')
    {{ declare_members(ROOT) | indent }}
    {{ init_members(ROOT) | indent }}
{{ gen_properties(ROOT) }}
{{ gen_writer(ROOT) }}
{{ gen_validator(ROOT) }}

    # overload
    def save(self, *args):
        '''
        Save changes you have made to the Snirf object to disk. If a filepath is supplied, the changes will be
        'saved as' in a new file.
        '''
        if len(args) > 0 and type(args[0]) is str:
            path = args[0]
            if not path.endswith('.snirf'):
                path += '.snirf'
            new_file = h5py.File(path, 'w')
            self._save(new_file)
            self._cfg.logger.info('Saved Snirf file at %s to copy at %s', self.filename, path)
            new_file.close()
        else:
            self._save(self._h.file)

    def validate(self) -> Tuple[bool, ValidationResult]:
        '''
        Returns a bool representing the validity of the Snirf object and the
        detailed output structure ValidationResult instance
        '''
        result = ValidationResult()
        self._validate(result)
        return (result.is_valid(), result)

    def close(self):
        self._cfg.logger.info('Closing Snirf file %s', self.filename)
        self._h.close()

    def __enter__(self):
        return self

#    def __del__(self):
#        self.close()

    def __getitem__(self, key):
        if self._h != {}:
            if key in self._h:
                return self._h[key]
        else:
            return None

{{ FOOTER }}